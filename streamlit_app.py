from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, mean_squared_error, r2_score
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from pathlib import Path

from streamlit_utils.streamlit_utils import load_classification_data, load_regression_data, plotly_map, fixNaN, get_lieux_compteurs_df, train_classification_model, train_regression_model
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
import plotly.express as px

# Configuration des chemins pour acc√©der aux mod√®les pr√©-entra√Æn√©s
MODELS_DIR = Path("models")

@st.cache_data
def load_raw_data():
    df_2023 = pd.read_csv('data/raw/velo_2023.csv', sep=';')
    df_2024 = pd.read_csv('data/raw/velo_2024.csv', sep=';')
    df = pd.concat([df_2023, df_2024], axis=0)
    df = fixNaN(df)
    return df

# Charger les donn√©es
df = pd.read_csv(r"data/processed/lieu-compteur-one-hot-encoded.csv", index_col=0)

st.image("streamlit_assets/banniere6.jpeg", use_container_width=True)

#Titres
st.title("**Projet Data Science - Trafic Cycliste √† Paris**")
st.subheader("_Donn√©es de Janvier 2023 √† F√©vrier 2025_")

st.sidebar.title("Sommaire")
pages=["Pr√©sentation du Projet", "Pr√©sentation du dataset", "Pr√©processing", "Visualisation des donn√©es", "Mod√©lisation", "Interpr√©tation et r√©sultats", "Conclusion"]
page=st.sidebar.radio("Aller vers", pages)
st.sidebar.write("""
_Projet r√©alis√© par:_
- _Alexandre COURROUX_
- _K√©vin LAKHDAR_
- _Ketsia PEDRO_
- _Eliah REBSTOCK_

_Bootcamp Machine Learning Engineer Janvier 2025_
""")


## Pr√©sentation du projet
if page == pages[0]:
    st.header("Pr√©sentation du projet", divider=True)

    st.header("I. Contexte")

    st.markdown("""
Face √† l'essor du v√©lo comme mode de transport durable, la Ville de Paris a mis en place, depuis plusieurs ann√©es, un r√©seau de compteurs √† v√©lo permanents pour mesurer l'√©volution de la pratique cycliste dans la capitale.

Ces capteurs, install√©s sur les axes cl√©s de la ville, collectent en continu des donn√©es sur le flux des cyclistes.
    """)

    st.image("streamlit_assets/comptagev√©lo.jpeg", use_container_width=True)

    st.markdown("""
Ce projet s'inscrit dans une d√©marche de transition vers une mobilit√© plus verte et une volont√© d'adapter les infrastructures urbaines aux besoins r√©els, tel que propos√© dans le plan v√©lo 2021-2026 d'am√©nagement de pistes cyclables de la mairie de Paris.

L'enjeu est de transformer ces donn√©es brutes en insights exploitables, permettant d'√©clairer les d√©cisions publiques de mani√®re objective et data-driven.
    """)

    st.divider()

    st.header("II. Objectifs")
    st.markdown("Ce projet vise √† d√©velopper un **outil pr√©dictif du trafic cycliste √† Paris**, en exploitant les donn√©es historiques des compteurs v√©lo.")

    st.subheader("Objectifs principaux")
    st.markdown("""
- Identifier les **tendances d‚Äôusage** (heures de pointe, zones satur√©es, variations saisonni√®res).
- G√©n√©rer des **visualisations claires** (cartes thermiques, graphiques temporels).
- Aider √† la **prise de d√©cision** sur les am√©nagements √† prioriser.
""")

    st.subheader("B√©n√©fices pour la Mairie de Paris :")
    st.markdown("""
- Prioriser les **am√©nagements cibl√©s** (pistes √©largies, carrefours s√©curis√©s, nouveaux itin√©raires).
- √âvaluer l‚Äô**impact des politiques existantes**.
- **Optimiser le r√©seau cyclable** √† long terme.
""")

    st.subheader("Ambition finale")
    st.markdown("> R√©duire les **conflits d‚Äôusage**, am√©liorer la **s√©curit√©**, et encourager la pratique du v√©lo gr√¢ce √† une **planification data-driven**, combinant **r√©trospective** et **pr√©diction** pour une mobilit√© plus fluide et r√©siliente.")








## Pr√©sentation du dataset
if page == pages[1]:
    st.header("Pr√©sentation du dataset", divider=True)
    st.header("1. Sources des donn√©es")
    st.image("streamlit_assets/opendata2.png", use_container_width=True)
    st.markdown("""
Utilisation des jeux de donn√©es ouverts propos√©s par la Ville de Paris via [opendata.paris.fr](https://opendata.paris.fr) :

  - le jeu de donn√©es [Comptage v√©lo - Donn√©es compteurs](https://opendata.paris.fr/explore/dataset/comptage-velo-donnees-compteurs/information) pour les donn√©es de 2024-2025.
  - le jeu de donn√©es [Comptage v√©lo - Historique - Donn√©es Compteurs et Sites de comptage](https://opendata.paris.fr/explore/dataset/comptage-velo-historique-donnees-compteurs/information) pour les donn√©es de 2023.

Les donn√©es sont publi√©es sous la licence Open Database License (ODbL), qui autorise la r√©utilisation, l‚Äôadaptation et la cr√©ation de travaux d√©riv√©s √† partir de ces jeux de donn√©es, √† condition d‚Äôen citer la source.
""")

    st.divider()

    st.header("2. P√©riode ")
    st.markdown("""
Les donn√©es sont mises √† jour quotidiennement.

Nous avons r√©cup√©r√© toutes les donn√©es du 1er janvier 2023 au 28 f√©vrier 2025 (26 mois).
""")

    st.divider()

    st.header("3. Contenu des jeux de donn√©es  ")
    st.markdown("""
Les jeux de donn√©es recensent les comptages horaires des passages de v√©los effectu√©s par environ une centaine de compteurs r√©partis dans la ville de Paris.

Chaque lieu de comptage est g√©n√©ralement √©quip√© de deux compteurs, face √† face, positionn√©s pour mesurer le trafic dans chaque direction d‚Äôune m√™me rue.

Au total, pour la p√©riode 2023-2024, le jeu de donn√©es contient environ 1,8 million de lignes r√©parties sur 16 variables.
""")

    st.divider()

    st.header("4. Structure des donn√©es")
    st.markdown("""
Chaque ligne du dataset correspond au nombre de v√©los enregistr√©s pendant une heure par un compteur donn√©.

Les donn√©es incluent, en plus du comptage horaire, plusieurs m√©tadonn√©es associ√©es au compteur ou au site de comptage, telles que :
- Le nom et l‚Äôidentifiant du compteur
- Le nom du lieu de comptage (en g√©n√©ral n¬∫ + rue)
- La date d‚Äôinstallation du compteur
- Les coordonn√©es g√©ographiques
- √âventuellement, des liens vers des photos du site
""")

    st.divider()
    st.header("5. Nettoyage et s√©lection des variables  ")

    st.markdown("""
Afin de simplifier et d‚Äôoptimiser l‚Äôanalyse, nous avons supprim√© les variables non pertinentes pour l'entra√Ænement du mod√®le, en particulier les champs techniques ou visuels comme les liens vers les photos, les identifiants internes ou le type d‚Äôimage.

Voici un extrait de notre dataset avec les variables que nous avons d√©cid√© de conserver :
""")

    st.dataframe(load_raw_data().sample(5))

    st.divider()

    st.header("6. Objectif d‚Äôanalyse et variable cible  ")
    st.markdown("""
L‚Äôobjectif de notre √©tude est de pr√©dire le nombre de v√©los compt√©s pendant une heure sur un compteur donn√©.
La variable cible de notre mod√®le est donc le comptage horaire, un indicateur cl√© pour analyser l'√©volution de la circulation cycliste dans Paris.
""")

    st.divider()
    st.header("7. Forces et limites du dataset")

    st.markdown("""
Le dataset se distingue par sa pr√©cision horaire et sa couverture g√©ographique dense, ce qui permet d‚Äôidentifier des tendances temporelles comme les variations quotidiennes ou saisonni√®res du trafic cycliste.

Cependant, il ne contient pas d‚Äôinformations contextuelles telles que :
- La m√©t√©o
- La pr√©sence d‚Äô√©v√©nements particuliers (manifestations, gr√®ves, festivals)
- Ou des donn√©es sociod√©mographiques comme la densit√© de population par zone

Cette absence limite la profondeur des analyses pr√©dictives que l‚Äôon peut mener.
""")

## Pr√©processing
if page == pages[2]:
    st.header("Pr√©processing des donn√©es", divider=True)
    st.header("1. Suppression des NaN")

    st.markdown("""
    Certaines variables de m√©tadonn√©es des compteurs ("Identifiant du compteur", "Coordonn√©es g√©ographiques", ...) ont des valeurs NaN (environ 3.4% sur le dataset).

    Plusieurs compteurs du dataset correspondent en r√©alit√© √† un m√™me emplacement, ce qui a permis de r√©duire les NaN en les renommant et fusionnant.

    Les derniers NaN provenaient de deux compteurs atypiques, finalement supprim√©s pour obtenir un dataset complet et sans valeurs manquantes.
    """)

    st.header("2. Conversion Date au format datetime")

    st.markdown("""
    Variable "Date et heure de comptage" convertie au format datetime de Pandas, en utilisant le fuseau horaire Europe/Paris afin de correctement capturer les tendances journali√®res.
    """)

    st.code("""
    # Convertir la colonne en datetime (avec gestion du fuseau horaire)
    df['Date et heure de comptage'] = pd.to_datetime(df['Date et heure de comptage'], utc=True)
    df['Date et heure de comptage'] = df['Date et heure de comptage'].dt.tz_convert("Europe/Paris")
    """, language="python")

    st.header("3. Ajout de variables")
    st.markdown("""
    La variable "Date et heure de comptage" d√©compos√©e en variables "ann√©e",
    "mois", "jour", "jour de la semaine" et "heure" afin de faciliter la data visualisation et voir si
    certaines de ces variables √©taient corr√©l√©s √† notre variable cible.
    """)

    st.code("""
    df["Jour"] = df["Date et heure de comptage"].dt.date
    df["Mois"] = df["Date et heure de comptage"].dt.month
    df["Ann√©e"] = df["Date et heure de comptage"].dt.year
    df["Heure"] = df["Date et heure de comptage"].dt.hour
    """, language="python")

    st.markdown('Ajout des variables cat√©gorielles binaires "Week-end", "Jour f√©ri√©s" et "Vacances scolaires" afin de mesurer si les jours non travaill√©s ont un impact sur la pratique cyclable.')

    st.header("4. Normalisation des donn√©es")

    st.markdown("""
    Nous avons appliqu√© deux types de **normalisation** sur les colonnes temporelles et contextuelles, notamment pour r√©duire l'impact des valeurs extr√™mes de Comptage horaire sur les pr√©dictions de notre mod√®le, la variable Comptage horaire ne suivant pas une loi normale :

    1. **Standardisation** : centre les donn√©es autour de 0 avec une variance de 1.
    2. **Min-Max Scaling** : transforme les valeurs dans une plage d√©finie, ici entre 0 et 1.
    """)

    st.subheader("üîπ Standardisation")

    st.code("""
    from sklearn.preprocessing import StandardScaler

col_norm = ["Jour", "Mois", "Ann√©e", "Heure", "Jour_semaine", "Jour f√©ri√©", "Vacances scolaires"]

scaler = StandardScaler()
df[col_norm] = scaler.fit_transform(df[col_norm])
    """, language="python")

    st.subheader("üîπ Normalisation Min-Max")

    st.code("""
from sklearn.preprocessing import MinMaxScaler

col_norm = ["Jour", "Mois", "Ann√©e", "Heure", "Jour_semaine", "Jour f√©ri√©", "Vacances scolaires"]

scaler = MinMaxScaler(feature_range=(0, 1))
df[col_norm] = scaler.fit_transform(df[col_norm])
    """, language="python")

    st.markdown("""
    Ces transformations permettent de pr√©parer les donn√©es pour les mod√®les sensibles √† l‚Äô√©chelle des variables (r√©gressions, KNN, etc.).
    """)

    st.header("Extrait du Dataframe apr√®s pr√©-processing")
    st.dataframe(df.head(10))




## Visualisation des donn√©es
if page == pages[3]:
    raw_data = load_raw_data()

    st.header("Visualisation des donn√©es", divider=True)

    st.header("I. Distribution de la variable cible")
    st.subheader('Boxplot global de la variable comptage horaire')

    fig = plt.figure(figsize=(10, 5))
    sns.boxplot(raw_data, x='Comptage horaire')
    st.pyplot(fig)

    st.markdown('On remarque la pr√©sence de nombreuses valeurs extr√™mes, au sens statistique. 75% des valeurs sont inf√©rieures √† 95 comptages par heure et 25% des valeurs sont inf√©rieures √† 11.')

    st.subheader('Histogramme de la variable comptage horaire')

    fig = plt.figure(figsize=(10, 5))
    sns.histplot(raw_data, x='Comptage horaire', kde=True)
    plt.xlim(0, 2000)
    st.pyplot(fig)

    st.markdown("L'histogramme confirme le nombre √©lev√© de valeurs inf√©rieures √† 100.")

    st.subheader("QQ-Plot de la variable comptage horaire")
    import statsmodels.api as sm

    fig = sm.qqplot(df["Comptage horaire"], line = "r")
    plt.title("QQ-Plot de Comptage horaire")
    st.pyplot(fig)

    st.markdown("""
    On peut se rendre compte que la distribution de la variable n'est pas normale.
    Ceci peut s'expliquer par le fait que les donn√©es soient concentr√©es vers 0.
    """)

    st.subheader('Boxplot de la variable comptage horaire en fonction du lieu de comptage')

    agg_data = get_lieux_compteurs_df(raw_data)
    site = st.selectbox("Nom du site de comptage", list(agg_data['Nom du site de comptage'].unique()))
    fig = plt.figure(figsize=(10, 5))
    sns.boxplot(agg_data.loc[agg_data['Nom du site de comptage'] == site], x='Comptage horaire')
    plt.title(site)
    st.pyplot(fig)
    st.header("II. Cartographie")
    st.markdown("Carte de la ville de Paris repr√©sentant les positions des diff√©rents compteurs du dataset. La taille de chaque point correspond au comptage horaire total sur la p√©riode 2023-2024.")

    st.plotly_chart(plotly_map(load_raw_data()))

    st.markdown("""
    Les compteurs sont r√©partis sur les axes principaux :

    - Sud-Ouest / Nord-Est (avenue Denfert-Rochereau et boulevard S√©bastopol)
    - Est-Ouest (avenue de la Grande Arm√©e et avenue des Champs-√âlys√©es).
    - Les quais de la Seine ainsi que le boulevard p√©riph√©rique Sud (le long de la voie de tram T3a) sont aussi couverts.

    Le boulevard p√©riph√©rique nord et les 17 et 18e arrondissements n'ont pas de compteurs.

    Les compteurs "centraux" ont plus de passages que ceux en p√©riph√©rie de Paris : il y a donc une corr√©lation entre la localisation du compteur et le comptage horaire.""")

    st.header("III. √âvolution temporelle")

    st.subheader("""a. √âvolution globale du trafic""")
    fig = plt.figure(figsize=(12, 5))
    comptage_quotidien = raw_data.groupby("date")["Comptage horaire"].sum()
    plt.plot(comptage_quotidien.index.astype(str), comptage_quotidien.values, linestyle = "-", color = "orange")

    plt.xlabel("Jour")
    plt.ylabel("Nombre de v√©los")
    plt.title("√âvolution du comptage quotidien des v√©los")
    plt.xticks(ticks = range(0, len(comptage_quotidien), 15), labels = comptage_quotidien.index[::15].astype(str), rotation = 45, fontsize = 8)

    plt.grid(axis = "y", linestyle = "--", alpha = 0.7)
    st.pyplot(fig)

    st.markdown("""
On peut observer ici certaines tendances notamment des diminutions significatives
au mois d'Ao√ªt et D√©cembre qui correspondent respectivement √† une saison chaude
pendant les vacances scolaires (voyages, etc.) et √† No√´l (saison plus froide et familiale).
Il semble √©galement y avoir une reprise au mois de Septembre montrant la reprise du travail
et la rentr√©e pour les √©tudiants.
""")

    st.subheader("""b. Saisonnalit√© du trafic""")

    fig = plt.figure()
    sns.barplot(df, x='Mois', y='Comptage horaire', errorbar=None)
    plt.xlabel("Mois")
    plt.xticks(rotation=45)
    plt.title("Comptage horaire moyen en fonction du mois")
    st.pyplot(fig)

    st.markdown("""
    On constate une baisse du comptage en **hiver** (janvier et d√©cembre) et en **√©t√©** (au mois d'ao√ªt).

    Cela est peut-√™tre d√ª aux **vacances**, √† certains **√©v√©nements** (JO de Paris en ao√ªt en 2024) et √† la **m√©t√©o** (il fait plus froid en hiver, ce qui n'encourage pas la pratique cycliste).""")

    st.subheader("""c. Comportement selon les jours""")

    fig = plt.figure(figsize=(10, 5))
    sns.barplot(df, x='Jour_semaine', y='Comptage horaire', errorbar=None)
    plt.xlabel("Jour du mois")
    plt.title("Comptage horaire moyen en fonction du jour de la semaine")
    st.pyplot(fig)

    st.markdown("""
    On constate √©galement plus de **comptages** du **lundi au vendredi**, ce qui correspond aux **trajets domicile-travail**.

    En moyenne, il y a environ **50% de v√©los en plus** en **semaine** par rapport au **week-end**.
    """)


    st.subheader("""d. Evolution du trafic au fil des heures""")

    st.markdown("""√Ä gauche : **jours de la semaine** (lundi √† vendredi) ‚Äî √Ä droite : **week-end** (samedi et dimanche)""")

    # Filtrage
    df_semaine = df[df['Jour_semaine'].isin([1, 2, 3, 4, 5])]
    df_weekend = df[df['Jour_semaine'].isin([6, 7])]

    # D√©termination du m√™me axe Y pour les deux graphiques
    y_max = max(df['Comptage horaire'].max(), 1)  # max global pour fixer l'√©chelle

    # Cr√©ation des deux graphiques c√¥te √† c√¥te
    fig, axes = plt.subplots(1, 2, figsize=(20, 7), sharey=True)

    # Graphique semaine
    sns.barplot(ax=axes[0], data=df_semaine, x='Heure', y='Comptage horaire', errorbar=None)
    axes[0].set_title("Comptage horaire moyen (lundi √† vendredi)")
    axes[0].set_xlabel("Heure de la journ√©e")
    axes[0].set_ylabel("Comptage horaire")
    axes[0].set_ylim(0, 400)

    # Graphique week-end
    sns.barplot(ax=axes[1], data=df_weekend, x='Heure', y='Comptage horaire', errorbar=None)
    axes[1].set_title("Comptage horaire moyen (week-end)")
    axes[1].set_xlabel("Heure de la journ√©e")
    axes[1].set_ylabel("")  # on peut laisser vide pour all√©ger visuellement
    axes[1].set_ylim(0, 400)

    st.pyplot(fig)

    st.markdown("""
    **Forte augmentation du trafic** aux **heures de pointe** (8h‚Äì9h / 18h‚Äì19h) en semaine.

    **Volume de passages** relativement **r√©gulier** entre **11h et 20h** le **week-end**.
    """)

    st.header("IV. Corr√©lation entre les variables")

    st.subheader("Matrice de corr√©lation entre les variables")

    st.image("streamlit_assets/matrice.jpeg", use_container_width=True)

    st.markdown("""
    * Le **comptage horaire** est l√©g√®rement corr√©l√© au **nom du compteur** et √† **l'heure de la journ√©e**.
    * Corr√©lation forte entre les variables **jour_semaine** et **week-end** (variables redondantes).
    * Forte corr√©lation entre **"date et heure de comptage"**, **ann√©e** et **mois**, ce qui semble logique, l'ann√©e et le mois √©tant d√©riv√©s de la date.
    """)


# Mod√©lisation
if page == pages[4]:
    st.header("Mod√©lisation", divider=True)
    problem_type = st.segmented_control("Type de probl√®me", ["Classification", "R√©gression"])

    if problem_type == 'Classification':
        # Chargement des donn√©es
        class_df = load_classification_data()
        X = class_df.drop(columns=["Comptage horaire"])
        col_norm = ["Jour", "Mois", "Ann√©e", "Heure", "Jour_semaine", "Jour f√©ri√©", "Vacances scolaires"]

        # Encodage des features
        encoder = OneHotEncoder(sparse_output=False, dtype=int)
        array = encoder.fit_transform(X[col_norm])
        encoded_df_clean = pd.DataFrame(array, columns=encoder.get_feature_names_out(col_norm))
        encoded_df_clean.index = X.index
        X_clean = pd.concat([X.drop(columns=col_norm), encoded_df_clean], axis=1)

        # Encodage variable cible
        label_enc = LabelEncoder()
        y = class_df["Comptage horaire"]
        y = label_enc.fit_transform(y)
        X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=42)

        st.header("Analyse de Classification")
        st.write("**Mod√®le choisi** : `RandomForestClassifier`")
        st.write("Nous avons divis√© comptage horaire selon ces intervalles :")
        st.write(sorted(class_df["Comptage horaire"].unique()))

        with st.expander("Hyperparam√®tres de Classification", expanded=True):
            n_estimators = st.slider("n_estimators", 50, 200, 200, 50)
            max_depth = st.slider("max_depth", 10, 100, 70, 10)
            params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'criterion': 'gini', 'min_samples_split':15, 'min_samples_leaf':2, 'max_features':'sqrt'}

            # On g√©n√®re un nom de fichier unique qui est bas√© sur les hyperparam√®tres
            model_filename = MODELS_DIR / f"rf_classifier_{n_estimators}_{max_depth}.pkl"

            # Ici on v√©rifie si le mod√®le est d√©j√† entra√Æn√©
            if model_filename.exists():
                st.info("Chargement du mod√®le pr√©-entra√Æn√©...")
                model = joblib.load(model_filename)
                st.success("Mod√®le charg√©.")
            else:
                st.info("Entra√Ænement du mod√®le... (peut prendre quelques minutes)")
                model = train_classification_model(X_train, y_train, params)
                joblib.dump(model, model_filename)
                st.success("Mod√®le entra√Æn√© et sauvegard√© pour une utilisation future !")

        # Pr√©dictions et √©valuation
        y_pred = model.predict(X_test)

        st.subheader("Performance du Mod√®le")
        st.dataframe(pd.DataFrame(classification_report(y_test, y_pred, target_names=label_enc.classes_, output_dict=True)).transpose())

        fig = px.imshow(confusion_matrix(y_test, y_pred),
                        labels=dict(x="Pr√©dit", y="R√©el", color="Count"),
                        x=label_enc.classes_, y=label_enc.classes_,
                        color_continuous_scale='Blues')
        fig.update_layout(title="Matrice de confusion")
        st.plotly_chart(fig, use_container_width=True)

        feature_importance = pd.DataFrame({
            'feature': X_test.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False).head(10)

        fig = px.bar(feature_importance, x='importance', y='feature',
                     title='Top 10 des caract√©ristiques importantes')
        st.plotly_chart(fig, use_container_width=True)

    if problem_type == 'R√©gression':
        # Chargement des donn√©es
        reg_df = load_regression_data()
        X = reg_df.drop(columns=["Comptage horaire"])
        col_norm = ["Jour", "Mois", "Ann√©e", "Heure", "Jour_semaine", "Jour f√©ri√©", "Vacances scolaires"]

        # Encodage des features
        encoder = OneHotEncoder(sparse_output=False, dtype=int)
        array = encoder.fit_transform(X[col_norm])
        encoded_df_clean = pd.DataFrame(array, columns=encoder.get_feature_names_out(col_norm))
        encoded_df_clean.index = X.index
        X_clean = pd.concat([X.drop(columns=col_norm), encoded_df_clean], axis=1)

        # Transformation logarithmique de la variable cible pour √©viter les valeurs aberrantes
        y = np.log1p(reg_df["Comptage horaire"])  # log(1+x) pour g√©rer les z√©ros
        X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=42)

        st.header("Analyse de R√©gression")
        st.write("""
        **Mod√®le choisi:** `HistGradientBoostingRegressor`

        **Note:** Une transformation logarithmique a √©t√© appliqu√©e √† la variable cible pour:
        - R√©duire l'impact des valeurs extr√™mes
        - √âviter les pr√©dictions n√©gatives
        - Normaliser la distribution des donn√©es
        """)

        with st.expander("Hyperparam√®tres de R√©gression", expanded=True):
            learning_rate = st.slider("Taux d'apprentissage", 0.1, 0.5, 0.5, 0.1)
            max_iter = st.slider("Nombre d'it√©rations", 50, 500, 500, 50)
            params = {'learning_rate': learning_rate, 'max_iter': max_iter}

            model_filename = MODELS_DIR / f"hgb_regressor_{learning_rate}_{max_iter}.pkl"

            # Ici on v√©rifie si le mod√®le est d√©j√† entra√Æn√©
            if model_filename.exists():
                st.info("Chargement du mod√®le pr√©-entra√Æn√©...")
                model = joblib.load(model_filename)
                st.success("Mod√®le charg√©.")
            else:
                st.info("Entra√Ænement du mod√®le... (peut prendre quelques minutes)")
                model = train_regression_model(X_train, y_train, params)
                joblib.dump(model, model_filename)
                st.success("Mod√®le entra√Æn√© et sauvegard√© pour une utilisation future !")

        y_pred = model.predict(X_test)

        # Conversion inverse des pr√©dictions (expm1 pour inverser log1p)
        y_test_exp = np.expm1(y_test)
        y_pred_exp = np.expm1(y_pred)

        st.subheader("Performance du Mod√®le")
        col1, col2, col3 = st.columns(3)
        col1.metric("RMSE", f"{np.sqrt(mean_squared_error(y_test_exp, y_pred_exp)):.2f}")
        col2.metric("R¬≤", f"{r2_score(y_test_exp, y_pred_exp):.2f}")
        col3.metric("MAE", f"{mean_absolute_error(y_test_exp, y_pred_exp):.2f}")

        fig = px.scatter(x=y_test_exp, y=y_pred_exp,
                         labels={'x': 'Valeurs R√©elles', 'y': 'Pr√©dictions'},
                         title='Valeurs R√©elles vs Pr√©dictions (√©chelle originale)')
        fig.add_shape(type='line', line=dict(dash='dash'),
                      x0=min(y_test_exp), y0=min(y_test_exp),
                      x1=max(y_test_exp), y1=max(y_test_exp))
        st.plotly_chart(fig, use_container_width=True)

        residuals = y_test_exp - y_pred_exp
        fig = px.histogram(residuals, nbins=500,
                           title='Distribution des R√©sidus',
                           labels={'value': 'R√©sidu'}, range_x=(-200, 200))
        st.plotly_chart(fig, use_container_width=True)


## Interpr√©tation et r√©sultats
if page == pages[5]:
    st.header("Interpr√©tation et r√©sultats", divider=True)
    st.header("Analyse comparative des erreurs")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Classification")
        st.write("Meilleures m√©triques obtenues (`n_estimators=200, max_depth=100`) :")
        st.code("""
        - Accuracy : 0.55
        - Pr√©cision moyenne : 0.56
        - Rappel moyen : 0.57
        - f1-score moyen : 0.56
        """)

    with col2:
        st.subheader("R√©gression")
        st.write("Meilleures m√©triques obtenues (`learning_rate=0.1, max_iter=5000`):")
        st.code("""
        - RMSE : 21.99
        - R¬≤ : 0.96
        """)

    st.header("Interpr√©tabilit√©")
    st.subheader("Importances des features pour le mod√®le HistGradientBoostingRegressor")

    st.image("streamlit_assets/feature importances.png", use_container_width=True)

    st.markdown("""
Nous pouvons constater ici que la caract√©ristique la plus importante pour le mod√®le est
l‚Äôheure et plus largement tout ce qui se rapporte √† la date (heure, mois, jour de la semaine
et ann√©e). Cela reste coh√©rent avec l‚Äôobjectif fix√© qui est de pr√©dire le nombre de v√©los sur
un compteur √† une heure pr√©cise.
""")

    st.header("Conclusions")

    st.subheader("Classification")
    st.image("streamlit_assets/erreurs_classes.png", use_container_width=True)
    st.write("""
    - Les classes extr√™mes (faible et haut trafic) sont les mieux pr√©dites.
    - Les classes interm√©diaires sont moins biens pr√©dites √† cause d'erreurs de classement entre classes voisines.
    - L'heure et le jour de la semaine sont les facteurs les plus d√©terminants.
    """)

    st.subheader('R√©gression')
    st.write("""
    - Bonnes performances globales avec un R¬≤ de 0.96.
    - Les erreurs augmentent lors des pics de trafic exceptionnels.
    - Le mod√®le capture bien les variations saisonni√®res horaires.
    """)

## Conclusion
if page == pages[6]:
    st.header("Synth√®se du Projet", divider=True)

    st.markdown("""
    Ce projet de data science vise √† **pr√©dire le trafic cycliste √† Paris** √† l'aide des donn√©es ouverte de comptage horaire.
    L'objectif principal √©tait de d√©velopper un mod√®le capable d'estimer avec pr√©cision le nombre de v√©los circulant
    sur les axes cyclables parisiens, afin d'aider la ville dans sa politique d'am√©nagement urbain.

    **Principales r√©alisations :**

    - Collecte et traitement de **1,8 million d'observations** (2023-2025)
    - Analyse exploratoire approfondie des tendances du trafic cycliste.
    - D√©veloppement de deux approches :
        - **Mod√©lisation par r√©gression** pour une pr√©diction pr√©cise du nombre de v√©los.
        - **Classification** pour cat√©goriser l'intensit√© du trafic.
    - Cr√©ation d'une application interactive pour visualiser les r√©sultats.
    """)

    st.divider()

    st.header("Principaux Enseignements")
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("üìà Insights Cl√©s")
        st.markdown("""
        - L'**heure** et le **jour de la semaine** sont les facteurs les plus pr√©dictifs.
        - Forte variation entre heures de pointe et p√©riodes creuses.
        - Impact visible des **vacances scolaires** et jours f√©ri√©s.
        - Diff√©rences marqu√©es selon les **localisations g√©ographiques**.
        """)

    with col2:
        st.subheader("‚öôÔ∏è Performance des Mod√®les")
        st.markdown("""
        - **R√©gression** (meilleure performance):
            - Pr√©cision moyenne : **¬±10 v√©los/heure**
            - Capacit√© √† capturer les tendances saisonni√®res
        - **Classification** :
            - Pr√©cision globale de **56%**
            - Bonne d√©tection des pics et creux de trafic
        """)

    st.divider()

    st.header("Applications Concr√®tes")
    st.markdown("""
    üîπ **Pour la Mairie de Paris :**

    - Optimisation des **am√©nagements cyclables**
    - Meilleure gestion des **flux aux heures de pointe**
    - Aide √† la d√©cision pour les **investissements infrastructurels**

    üîπ **Pour les Citoyens :**

    - Application potentielle pour **√©viter les axes satur√©s**
    - Visibilit√© sur les **tendances du trafic cycliste**
    """)

    st.divider()

    st.header("Perspectives d'Am√©liorations")
    st.markdown("""
    üöÄ **Am√©liorations Techniques:**

    - Int√©gration de donn√©es **m√©t√©orologiques**
    - Ajout d'informations sur les **√©v√©nements locaux**
    - Utilisation de techniques avanc√©es (deep learning, mod√®les s√©quentiels)

    üåç **Extensions Possibles:**

    - Pr√©diction √† l'√©chelle de la **semaine/mois**
    - Analyse comparative entre **diff√©rentes villes**
    - Syst√®me de recommandation d'itin√©raires cyclables
    """)

    st.divider()

    st.markdown("""
    <div style="background-color:#f0f2f6; padding:20px; border-radius:10px;">
    <h3 style="color:#1e88e5;">En r√©sum√©</h3>
    <p>Ce projet d√©montre la valeur des donn√©es de mobilit√© pour la gestion urbaine.
    Les r√©sultats obtenus ouvrent des perspectives int√©ressantes pour une ville comme Paris
    qui souhaite d√©velopper les mobilit√©s douces tout en optimisant ses infrastructures existantes.</p>
    </div>
    """, unsafe_allow_html=True)
