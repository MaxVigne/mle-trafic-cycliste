name: Train Model and test service pipeline

on:
  push:
    branches-ignore:
      - main

env:
  # github.repository as <account>/<repo>
  IMAGE_NAME: ${{ github.repository }}

jobs:
  process-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Retrieve 2024 raw data from opendata.paris.fr
        run: curl 'https://opendata.paris.fr/api/datasets/1.0/comptage-velo-historique-donnees-compteurs/attachments/2024_comptage_velo_donnees_compteurs_zip/' -o velo_2024.zip && unzip velo_2024.zip && mv 2024-comptage-velo-donnees-compteurs.csv data/raw/velo_2024.csv && rm velo_2024.zip

      - name: Retrieve 2023 raw data from opendata.paris.fr
        run: curl 'https://opendata.paris.fr/api/datasets/1.0/comptage-velo-historique-donnees-compteurs/attachments/2023_comptage_velo_donnees_compteurs_zip/' -o velo_2023.zip && unzip velo_2023.zip && mv 2023_comptage-velo-donnees-compteurs.csv data/raw/velo_2023.csv && rm velo_2023.zip

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version-file: ".python-version"

      - name: Generate processed data for model training
        run: uv run src/features/process_data.py

      - name: Upload dataset as artifact
        uses: actions/upload-artifact@v4
        with:
          name: velo_dataset
          path: data/processed/lieu-compteur-one-hot-encoded.csv

      - name: Upload one_hot_encoder as artifact
        uses: actions/upload-artifact@v4
        with:
          name: one_hot_encoder
          path: models/one_hot_encoder.pkl

  train-model:
    runs-on: ubuntu-latest
    needs: process-data
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download dataset as artifact
        uses: actions/download-artifact@v4
        with:
          name: velo_dataset
          path: data/processed

      - name: Download one_hot_encoder as artifact
        uses: actions/download-artifact@v4
        with:
          name: one_hot_encoder
          path: models

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version-file: ".python-version"

      - name: Train model and log results to MLflow
        run: uv run src/models/train_model.py
        env:
          MLFLOW_TRACKING_USERNAME: MaxVigne
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}

      - name: Upload model as artifact
        uses: actions/upload-artifact@v4
        with:
          name: hgb_regressor_model
          path: models/hgb_regressor_0.5_500.pkl

  build-and-test-service:
    runs-on: ubuntu-latest
    needs: [process-data, train-model]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download one_hot_encoder as artifact
        uses: actions/download-artifact@v4
        with:
          name: one_hot_encoder
          path: models

      - name: Download model as artifact
        uses: actions/download-artifact@v4
        with:
          name: hgb_regressor_model
          path: models

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version-file: ".python-version"

      # - name: Pull models data with dvc (required for docker build)
      #   run: uv run dvc pull

      - name: Run docker image with docker compose
        run: docker compose up -d

      - name: Run tests with pytest
        run: uv run pytest tests/*

      - name: Run docker compose down to cleanup containers
        if: always()
        run: docker compose down
